{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teststr = \"我们生活中经常会接触的自然语言处理的应用，包括语音识别，语音翻译，理解句意，理解特定词语的同义词，以及写出语法正确，句意通畅的句子和段落。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = jieba.cut(teststr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba.add_word(\"语音识别\")\n",
    "jieba.add_word(\"语音翻译\")\n",
    "jieba.add_word(\"自然语言处理\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们\n",
      "生活\n",
      "中\n",
      "经常\n",
      "会\n",
      "接触\n",
      "的\n",
      "自然语言处理\n",
      "的\n",
      "应用\n",
      "，\n",
      "包括\n",
      "语音识别\n",
      "，\n",
      "语音翻译\n",
      "，\n",
      "理解\n",
      "句意\n",
      "，\n",
      "理解\n",
      "特定\n",
      "词语\n",
      "的\n",
      "同义词\n",
      "，\n",
      "以及\n",
      "写出\n",
      "语法\n",
      "正确\n",
      "，\n",
      "句意\n",
      "通畅\n",
      "的\n",
      "句子\n",
      "和\n",
      "段落\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我们'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'flag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-92e339aa58b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'flag'"
     ]
    }
   ],
   "source": [
    "w.flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package jieba:\n",
      "\n",
      "NAME\n",
      "    jieba\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __main__\n",
      "    _compat\n",
      "    analyse (package)\n",
      "    finalseg (package)\n",
      "    posseg (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        Tokenizer\n",
      "    \n",
      "    class Tokenizer(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dictionary=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  add_word(self, word, freq=None, tag=None)\n",
      "     |      Add a word to dictionary.\n",
      "     |      \n",
      "     |      freq and tag can be omitted, freq defaults to be a calculated value\n",
      "     |      that ensures the word can be cut out.\n",
      "     |  \n",
      "     |  calc(self, sentence, DAG, route)\n",
      "     |  \n",
      "     |  check_initialized(self)\n",
      "     |  \n",
      "     |  cut(self, sentence, cut_all=False, HMM=True)\n",
      "     |      The main function that segments an entire sentence that contains\n",
      "     |      Chinese characters into seperated words.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - sentence: The str(unicode) to be segmented.\n",
      "     |          - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
      "     |          - HMM: Whether to use the Hidden Markov Model.\n",
      "     |  \n",
      "     |  cut_for_search(self, sentence, HMM=True)\n",
      "     |      Finer segmentation for search engines.\n",
      "     |  \n",
      "     |  del_word(self, word)\n",
      "     |      Convenient function for deleting a word.\n",
      "     |  \n",
      "     |  gen_pfdict(self, f)\n",
      "     |  \n",
      "     |  get_DAG(self, sentence)\n",
      "     |  \n",
      "     |  get_dict_file(self)\n",
      "     |  \n",
      "     |  initialize(self, dictionary=None)\n",
      "     |  \n",
      "     |  lcut(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  lcut_for_search(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  load_userdict(self, f)\n",
      "     |      Load personalized dict to improve detect rate.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - f : A plain text file contains words and their ocurrences.\n",
      "     |                Can be a file-like object, or the path of the dictionary file,\n",
      "     |                whose encoding must be utf-8.\n",
      "     |      \n",
      "     |      Structure of dict file:\n",
      "     |      word1 freq1 word_type1\n",
      "     |      word2 freq2 word_type2\n",
      "     |      ...\n",
      "     |      Word type may be ignored\n",
      "     |  \n",
      "     |  set_dictionary(self, dictionary_path)\n",
      "     |  \n",
      "     |  suggest_freq(self, segment, tune=False)\n",
      "     |      Suggest word frequency to force the characters in a word to be\n",
      "     |      joined or splitted.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - segment : The segments that the word is expected to be cut into,\n",
      "     |                      If the word should be treated as a whole, use a str.\n",
      "     |          - tune : If True, tune the word frequency.\n",
      "     |      \n",
      "     |      Note that HMM may affect the final result. If the result doesn't change,\n",
      "     |      set HMM=False.\n",
      "     |  \n",
      "     |  tokenize(self, unicode_sentence, mode='default', HMM=True)\n",
      "     |      Tokenize a sentence and yields tuples of (word, start, end)\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - sentence: the str(unicode) to be segmented.\n",
      "     |          - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n",
      "     |          - HMM: whether to use the Hidden Markov Model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    add_word(word, freq=None, tag=None) method of Tokenizer instance\n",
      "        Add a word to dictionary.\n",
      "        \n",
      "        freq and tag can be omitted, freq defaults to be a calculated value\n",
      "        that ensures the word can be cut out.\n",
      "    \n",
      "    calc(sentence, DAG, route) method of Tokenizer instance\n",
      "    \n",
      "    cut(sentence, cut_all=False, HMM=True) method of Tokenizer instance\n",
      "        The main function that segments an entire sentence that contains\n",
      "        Chinese characters into seperated words.\n",
      "        \n",
      "        Parameter:\n",
      "            - sentence: The str(unicode) to be segmented.\n",
      "            - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
      "            - HMM: Whether to use the Hidden Markov Model.\n",
      "    \n",
      "    cut_for_search(sentence, HMM=True) method of Tokenizer instance\n",
      "        Finer segmentation for search engines.\n",
      "    \n",
      "    del_word(word) method of Tokenizer instance\n",
      "        Convenient function for deleting a word.\n",
      "    \n",
      "    disable_parallel()\n",
      "    \n",
      "    enable_parallel(processnum=None)\n",
      "        Change the module's `cut` and `cut_for_search` functions to the\n",
      "        parallel version.\n",
      "        \n",
      "        Note that this only works using dt, custom Tokenizer\n",
      "        instances are not supported.\n",
      "    \n",
      "    get_DAG(sentence) method of Tokenizer instance\n",
      "    \n",
      "    get_FREQ lambda k, d=None\n",
      "    \n",
      "    get_dict_file() method of Tokenizer instance\n",
      "    \n",
      "    initialize(dictionary=None) method of Tokenizer instance\n",
      "    \n",
      "    lcut(*args, **kwargs) method of Tokenizer instance\n",
      "    \n",
      "    lcut_for_search(*args, **kwargs) method of Tokenizer instance\n",
      "    \n",
      "    load_userdict(f) method of Tokenizer instance\n",
      "        Load personalized dict to improve detect rate.\n",
      "        \n",
      "        Parameter:\n",
      "            - f : A plain text file contains words and their ocurrences.\n",
      "                  Can be a file-like object, or the path of the dictionary file,\n",
      "                  whose encoding must be utf-8.\n",
      "        \n",
      "        Structure of dict file:\n",
      "        word1 freq1 word_type1\n",
      "        word2 freq2 word_type2\n",
      "        ...\n",
      "        Word type may be ignored\n",
      "    \n",
      "    log(...)\n",
      "        log(x[, base])\n",
      "        \n",
      "        Return the logarithm of x to the given base.\n",
      "        If the base not specified, returns the natural logarithm (base e) of x.\n",
      "    \n",
      "    md5 = openssl_md5(...)\n",
      "        Returns a md5 hash object; optionally initialized with a string\n",
      "    \n",
      "    setLogLevel(log_level)\n",
      "    \n",
      "    set_dictionary(dictionary_path) method of Tokenizer instance\n",
      "    \n",
      "    suggest_freq(segment, tune=False) method of Tokenizer instance\n",
      "        Suggest word frequency to force the characters in a word to be\n",
      "        joined or splitted.\n",
      "        \n",
      "        Parameter:\n",
      "            - segment : The segments that the word is expected to be cut into,\n",
      "                        If the word should be treated as a whole, use a str.\n",
      "            - tune : If True, tune the word frequency.\n",
      "        \n",
      "        Note that HMM may affect the final result. If the result doesn't change,\n",
      "        set HMM=False.\n",
      "    \n",
      "    tokenize(unicode_sentence, mode='default', HMM=True) method of Tokenizer instance\n",
      "        Tokenize a sentence and yields tuples of (word, start, end)\n",
      "        \n",
      "        Parameter:\n",
      "            - sentence: the str(unicode) to be segmented.\n",
      "            - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n",
      "            - HMM: whether to use the Hidden Markov Model.\n",
      "\n",
      "DATA\n",
      "    DEFAULT_DICT = None\n",
      "    DEFAULT_DICT_NAME = 'dict.txt'\n",
      "    DICT_WRITING = {}\n",
      "    PY2 = False\n",
      "    __license__ = 'MIT'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    default_encoding = 'utf-8'\n",
      "    default_logger = <Logger jieba (DEBUG)>\n",
      "    dt = <Tokenizer dictionary=None>\n",
      "    log_console = <StreamHandler stderr (NOTSET)>\n",
      "    pool = None\n",
      "    re_eng = re.compile('[a-zA-Z0-9]')\n",
      "    re_han_cut_all = re.compile('([一-鿕]+)')\n",
      "    re_han_default = re.compile('([一-鿕a-zA-Z0-9+#&\\\\._%]+)')\n",
      "    re_skip_cut_all = re.compile('[^a-zA-Z0-9+#\\n]')\n",
      "    re_skip_default = re.compile('(\\r\\n|\\\\s)')\n",
      "    re_userdict = re.compile('^(.+?)( [0-9]+)?( [a-z]+)?$')\n",
      "    string_types = (<class 'str'>,)\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    user_word_tag_tab = {}\n",
      "\n",
      "VERSION\n",
      "    0.39\n",
      "\n",
      "FILE\n",
      "    d:\\users\\52489\\anaconda3\\lib\\site-packages\\jieba\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package jieba.posseg in jieba:\n",
      "\n",
      "NAME\n",
      "    jieba.posseg\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    char_state_tab\n",
      "    prob_emit\n",
      "    prob_start\n",
      "    prob_trans\n",
      "    viterbi\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        POSTokenizer\n",
      "        pair\n",
      "    \n",
      "    class POSTokenizer(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  __init__(self, tokenizer=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cut(self, sentence, HMM=True)\n",
      "     |  \n",
      "     |  initialize(self, dictionary=None)\n",
      "     |  \n",
      "     |  lcut(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  load_word_tag(self, f)\n",
      "     |  \n",
      "     |  makesure_userdict_loaded(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class pair(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __init__(self, word, flag)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  __unicode__(self)\n",
      "     |  \n",
      "     |  encode(self, arg)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    cut(sentence, HMM=True)\n",
      "        Global `cut` function that supports parallel processing.\n",
      "        \n",
      "        Note that this only works using dt, custom POSTokenizer\n",
      "        instances are not supported.\n",
      "    \n",
      "    initialize(dictionary=None) method of POSTokenizer instance\n",
      "    \n",
      "    lcut(sentence, HMM=True)\n",
      "    \n",
      "    load_model()\n",
      "\n",
      "DATA\n",
      "    CHAR_STATE_TAB_P = 'char_state_tab.p'\n",
      "    PROB_EMIT_P = 'prob_emit.p'\n",
      "    PROB_START_P = 'prob_start.p'\n",
      "    PROB_TRANS_P = 'prob_trans.p'\n",
      "    PY2 = False\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    char_state_tab_P = {'一': (('B', 'm'), ('S', 'm'), ('B', 'd'), ('B', 'a...\n",
      "    default_encoding = 'utf-8'\n",
      "    dt = <POSTokenizer tokenizer=<Tokenizer dictionary=None>>\n",
      "    emit_P = {('B', 'a'): {'一': -3.618715666782108, '万': -10.5005668853815...\n",
      "    re_eng = re.compile('[a-zA-Z0-9]+')\n",
      "    re_eng1 = re.compile('^[a-zA-Z0-9]$')\n",
      "    re_han_detail = re.compile('([一-鿕]+)')\n",
      "    re_han_internal = re.compile('([一-鿕a-zA-Z0-9+#&\\\\._]+)')\n",
      "    re_num = re.compile('[\\\\.0-9]+')\n",
      "    re_skip_detail = re.compile('([\\\\.0-9]+|[a-zA-Z0-9]+)')\n",
      "    re_skip_internal = re.compile('(\\r\\n|\\\\s)')\n",
      "    start_P = {('B', 'a'): -4.762305214596967, ('B', 'ad'): -6.68006603678...\n",
      "    string_types = (<class 'str'>,)\n",
      "    trans_P = {('B', 'a'): {('E', 'a'): -0.0050648453069648755, ('M', 'a')...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "FILE\n",
      "    d:\\users\\52489\\anaconda3\\lib\\site-packages\\jieba\\posseg\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pseg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teststr = \"请把空调打开\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'teststr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c7fe9d98844b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpseg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteststr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'teststr' is not defined"
     ]
    }
   ],
   "source": [
    "words = pseg.cut(teststr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请 v\n",
      "把 p\n",
      "空调 n\n",
      "打开 v\n"
     ]
    }
   ],
   "source": [
    "SegMap = dict()\n",
    "for w in words:\n",
    "    SegMap.setdefault(w.flag, set()).add(w.word)\n",
    "    print(w.word,w.flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': {'空调'}, 'p': {'把'}, 'v': {'打开', '请'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SegMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tairele = \"太热了\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = pseg.cut(tairele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\52489\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.940 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "太 d\n",
      "热 a\n",
      "了 ul\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(w.word, w.flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['太热', '了']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {\"reqType\":0,\n",
    "    \"perception\": {\n",
    "        \"inputText\": {\n",
    "            \"text\": \"今天天气不错\"\n",
    "        },\n",
    "        \"inputImage\": {\n",
    "            \"url\": \"imageUrl\"\n",
    "        },\n",
    "        \"selfInfo\": {\n",
    "            \"location\": {\n",
    "                \"city\": \"北京\",\n",
    "                \"province\": \"北京\",\n",
    "                \"street\": \"信息路\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"userInfo\": {\n",
    "        \"apiKey\": \"db22788e41ae4eb79445fb4a5e61c4ac\",\n",
    "        \"userId\": \"317370\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"http://openapi.tuling123.com/openapi/api/v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function post in module requests.api:\n",
      "\n",
      "post(url, data=None, json=None, **kwargs)\n",
      "    Sends a POST request.\n",
      "    \n",
      "    :param url: URL for the new :class:`Request` object.\n",
      "    :param data: (optional) Dictionary (will be form-encoded), bytes, or file-like object to send in the body of the :class:`Request`.\n",
      "    :param json: (optional) json data to send in the body of the :class:`Request`.\n",
      "    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n",
      "    :return: :class:`Response <Response>` object\n",
      "    :rtype: requests.Response\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(requests.post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = requests.post(url, json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"emotion\":{\"robotEmotion\":{\"a\":0,\"d\":0,\"emotionId\":0,\"p\":0},\"userEmotion\":{\"a\":0,\"d\":0,\"emotionId\":21500,\"p\":0}},\"intent\":{\"actionName\":\"\",\"code\":10004,\"intentName\":\"\"},\"results\":[{\"groupType\":1,\"resultType\":\"text\",\"values\":{\"text\":\"\\xe5\\xa4\\xa9\\xe5\\xa4\\xa9\\xe5\\xaf\\xb9\\xe7\\x9d\\x80\\xe7\\x94\\xb5\\xe8\\x84\\x91\\xe7\\xa0\\x81\\xe7\\xa8\\x8b\\xe5\\xba\\x8f\\xef\\xbc\\x8c\\xe7\\xb4\\xaf\\xe5\\x95\\x8a\\xef\\xbc\\x81\"}}]}'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emotion': {'robotEmotion': {'a': 0, 'd': 0, 'emotionId': 0, 'p': 0},\n",
       "  'userEmotion': {'a': 0, 'd': 0, 'emotionId': 10300, 'p': 0}},\n",
       " 'intent': {'actionName': '', 'code': 10004, 'intentName': ''},\n",
       " 'results': [{'groupType': 1,\n",
       "   'resultType': 'text',\n",
       "   'values': {'text': '开空调就要关上窗'}}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'心情好，自然天气好。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()['results'][0].get('values').get('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
